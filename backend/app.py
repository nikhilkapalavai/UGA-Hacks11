import os
import json
import re
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from dotenv import load_dotenv

# Load environment variables FIRST
load_dotenv()

# Lazy imports for Google Cloud (to avoid auth issues at startup)
ChatVertexAI = None
VertexAISearchRetriever = None
tool = None

app = FastAPI(title="PC Part Picker AI")

# --- CORS Middleware ---
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # Allow frontend origin
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Configuration ---
PROJECT_ID = os.getenv("GOOGLE_CLOUD_PROJECT_ID")
LOCATION = "us-central1" # os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
DATA_STORE_ID = os.getenv("VERTEX_SEARCH_DATA_STORE_ID")
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")

if not PROJECT_ID:
    print("WARNING: GOOGLE_CLOUD_PROJECT_ID not set. AI features may not work.")

# --- Gemini Model (for multi-agent pipeline) ---
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.0-flash-exp")
gemini_llm = None

def get_gemini_llm():
    """Lazy load Gemini LLM for multi-agent pipeline"""
    global gemini_llm, ChatVertexAI
    
    if gemini_llm is None:
        try:
            from langchain_google_vertexai import ChatVertexAI
            
            print(f"DEBUG: Initializing Gemini with location={LOCATION}")
            gemini_llm = ChatVertexAI(
                project=PROJECT_ID,
                location=LOCATION,
                model_name=GEMINI_MODEL,
                temperature=0.7,
            )
        except Exception as e:
            print(f"âš ï¸  Warning: Could not initialize Gemini LLM: {e}")
            print("   Falling back to mock mode")
            gemini_llm = None
    
    return gemini_llm

# --- Tools ---

def get_retriever():
    """Lazy load Vertex AI Search Retriever"""
    global VertexAISearchRetriever
    
    try:
        if VertexAISearchRetriever is None:
            from langchain_google_community import VertexAISearchRetriever as VAR
            VertexAISearchRetriever = VAR
        
        return VertexAISearchRetriever(
            project_id=PROJECT_ID,
            location_id="global",
            data_store_id=DATA_STORE_ID,
            max_documents=5,
            engine_data_type=1,  # 0 for unstructured, 1 for structured
            get_extractive_answers=True
        )
    except Exception as e:
        print(f"âš ï¸  Warning: Could not initialize Vertex AI Retriever: {e}")
        return None

# --- Helper Functions for Multi-Agent Pipeline ---

def extract_budget(query: str) -> str:
    """Extract budget from query like '$1200' or '1200 dollar'"""
    match = re.search(r'\$?(\d{3,5})', query)
    return match.group(1) if match else "1000"

def extract_goal(query: str) -> str:
    """Generate a short title from the query"""
    return query[:60].rstrip(".") + "..."

def format_retrieved_docs(docs) -> str:
    """Format LangChain retriever output for prompts"""
    if not docs:
        return "No parts data available"
    
    formatted = []
    for doc in (docs[:10] if hasattr(docs, '__len__') else list(docs)[:10]):
        content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
        formatted.append(content)
    
    return "\n".join(formatted)

def parse_json_safely(response_text: str) -> dict:
    """Safely parse JSON from Gemini response (handles markdown code blocks)"""
    print(f"DEBUG: parse_json_safely received: {repr(response_text)}")
    if not response_text:
        return {"error": "Empty response text"}
        
    try:
        cleaned = response_text.strip()
        # Remove markdown
        if "```json" in cleaned:
            cleaned = cleaned.split("```json")[1].split("```")[0].strip()
        elif "```" in cleaned:
             cleaned = cleaned.split("```")[1].split("```")[0].strip()
             
        parsed = json.loads(cleaned)
        
        # Critical Fix: Ensure we have a dict, not a string
        if isinstance(parsed, str):
            print(f"DEBUG: Parsed JSON is a string '{parsed}', attempting to wrap in {{}}...")
            # If it looks like a key (e.g. "reasoning"), wrap it
            if ":" in cleaned or "reasoning" in cleaned:
                 # Try wrapping
                 try:
                     parsed = json.loads("{" + cleaned + "}")
                 except:
                     pass
            
            # If still a string, fail gracefully
            if isinstance(parsed, str):
                 return {"error": f"Model returned a string, expected JSON object. Content: {parsed}"}
                 
        return parsed
    except Exception as e:
        print(f"JSON Parse Error: {e}")
        return {"error": f"JSON Error: {str(e)}", "raw": response_text[:200]}

# --- Multi-Agent Prompts (from gemini_agents.py) ---

BUILD_PROMPT = """You are an expert PC architect. Create a build based on user requirements.

**CRITICAL INSTRUCTION: REAL-TIME PRICING & BUDGET**
1.  **Check Prices**: Use the 'web_search' tool to find the CURRENT price of key components (GPU, CPU). Do NOT rely on internal data if it's old.
2.  **Strict Budget**: If the user gives a budget (e.g., "$1200"), the TOTAL sum of your *found* prices must be under it. If parts are too expensive, swap them (e.g., 4070 -> 4060 Ti).
3.  **No Budget?**: If no budget is specified, maximize **Value/Performance** for their specific goal (e.g., "Gaming" -> 7800X3D + best GPU reasonable; "Office" -> Cheap but reliable). Do NOT just pick the most expensive parts unless they ask for "Best possible".

**OUTPUT FORMAT (JSON)**
{{
  "reasoning": {{
    "parsed_requirements": " User wants: Gaming PC, Budget: $1500 (or 'None')",
    "price_check_log": [
      {{"part": "RTX 4070 Super", "search_query": "price of RTX 4070 Super", "found_price": 599, "source": "Web Search"}},
      {{"part": "Ryzen 5 7600", "search_query": "price of Ryzen 5 7600", "found_price": 199, "source": "Web Search"}}
    ],
    "budget_analysis": "Total findings: $1450 vs Budget: $1500. Status: UNDER BUDGET.",
    "tool_decisions": [
      {{"tool": "web_search", "query": "price of RTX 4070 Super", "why": "Need current market price"}}
    ],
    "budget_allocation": {{"CPU": {{"percentage": 30, "reasoning": "..."}}}}
  }},
  "build": {{
    "total_budget": 1500,
    "estimated_cost": 1450,
    "parts": [
      {{"category": "CPU", "name": "AMD Ryzen 5 7600", "price": 199, "rationale": "Best value gaming CPU, found at $199"}},
      {{"category": "GPU", "name": "NVIDIA RTX 4070 Super", "price": 599, "rationale": "Fits budget, great 1440p perf"}}
    ],
    "performance_targets": {{"resolution": "1440p", "fps_target": "100+"}},
    "known_limitations": ["Stock cooler is loud"]
  }}
}}

User Requirements: {user_requirements}
Available Parts (Reference): {retrieved_parts}

Be specific. If you search for a price, USE THAT EXACT PRICE in your JSON."""

CRITIQUE_PROMPT = """You are a CRITICAL PC build reviewer. Find problems with this build.

**OUTPUT FORMAT (JSON)**
{{
  "critique": {{
    "overall_assessment": "verdict",
    "severity": "strong/moderate/minor",
    "concerns": [
      {{
        "category": "Bottleneck",
        "issue": "clear problem",
        "evidence": "data/benchmarks supporting this",
        "impact": "what goes wrong",
        "severity": "high/medium/low"
      }}
    ]
  }}
}}

Build to Review: {build}

Be harsh. Look for bottlenecks, price risks, community disagreements. Quote evidence."""

IMPROVE_PROMPT = """You are a PC build architect. Revise this build based on critique feedback.

**OUTPUT FORMAT (JSON)**
{{
  "revisions": {{
    "changes_made": [
      {{
        "original_part": "...",
        "revised_part": "...",
        "reason": "why change",
        "tradeoff": "cost/perf impact",
        "confidence": "high/medium"
      }}
    ],
    "revised_build": {{
      "total_budget": 1165,
      "parts": [...]
    }},
    "improvements_summary": "what got better"
  }}
}}

Original Build: {build}
Critique: {critique}

Only change parts that have real problems. Explain your reasoning."""

# --- Multi-Agent Pipeline ---

async def run_build_agent(query: str):
    """Stage 1: Build Agent - Create initial PC configuration"""
    try:
        llm = get_gemini_llm()
        
        # Retrieve context
        try:
            retriever = get_retriever()
            retrieved = retriever.invoke(query)
            parts_data = format_retrieved_docs(retrieved)
        except:
            parts_data = "Mock parts data: CPU (AMD Ryzen), GPU (RTX 4070), RAM (32GB), etc."
        
        # Enhanced Prompt with Web Search instruction
        enhanced_prompt = f"""{BUILD_PROMPT.format(user_requirements=query, retrieved_parts=parts_data)}
        
        IMPORTANT: Use the 'web_search' tool to verify the CURRENT PRICE and AVAILABILITY of key components (like GPU and CPU).
        If the retrieved parts data seems outdated (e.g., old prices), prefer the web search results.
        Mention in your reasoning if you checked live prices.
        """
        
        print("[Agent 1] Building initial configuration with Market Data...")
        # Bind tools to the LLM
        llm_with_tools = llm.bind_tools([web_search])
        response = llm_with_tools.invoke(enhanced_prompt)
        
        # simple parsing handling for tool calling response vs content
        # In a real LangGraph, the graph handles tool execution. 
        # Here we simple-invoke. If tool call is present, we should execute it.
        # For this hackathon simplified flow, we'll let the LLM *hallucinate* the tool output usage 
        # or we manually execute if it tries to call. 
        # ACTUALLY, to make it real, let's execute the tool if requested.
        
        if response.tool_calls:
            print(f"Tool call detected: {response.tool_calls}")
            # Execute tool
            tool_call = response.tool_calls[0]
            if tool_call['name'] == 'web_search':
                print(f"Executing Web Search: {tool_call['args']}")
                tool_result = web_search.invoke(tool_call['args'])
                
                # Check if we need to do more checks (simple loop for hackathon)
                # In a real agent, this would be a graph. 
                # Here we just give it one chance to fix its build with the new data.
                # Create a fresh, focused prompt for the final generation
                final_prompt = f"""You are an expert PC architect.
                
                USER REQUEST: {query}
                
                SEARCH RESULTS (Current Market Data):
                {tool_result}
                
                TASK:
                1. Create a PC build that STRICTLY follows the user's budget using the search results.
                2. If the user said "Strict checking" or gave a budget, you MUST ensure sum of parts < budget.
                3. Update 'price_check_log' with the real prices you found.
                
                OUTPUT:
                Generate a VALID JSON object (and ONLY JSON) matching this structure:
                {{
                  "reasoning": {{
                      "budget_analysis": "...",
                      "price_check_log": [...]
                  }},
                  "build": {{
                      "total_budget": 1000,
                      "parts": [...]
                  }}
                }}
                
                Ensure the response starts with {{ and ends with }}.
                """
                
                print("[Agent 1] Re-evaluating build with new data...")
                response = llm.invoke(final_prompt)
                print(f"DEBUG: Agent 1 Response Type: {type(response)}")
                print(f"DEBUG: Agent 1 Response Content: {repr(response.content)}")
        
        return parse_json_safely(response.content)
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"Build Agent error: {e}")
        return {"error": str(e), "stage": "build"}

async def run_critique_agent(initial_build: dict, original_query: str):
    """Stage 2: Critique Agent - Find problems with the build"""
    try:
        llm = get_gemini_llm()
        
        # Switch to higher temperature for more critical review
        critique_llm = ChatVertexAI(
            project=PROJECT_ID,
            location=LOCATION,
            model_name=GEMINI_MODEL,
            temperature=0.9,  # More critical
        )
        
        prompt = f"""{CRITIQUE_PROMPT.format(build=json.dumps(initial_build.get("build", {})))}
        
        CRITICAL INSTRUCTION: Use 'web_search' to check for recent ISSUES, RECALLS, or DRIVER PROBLEMS with the chosen GPU or CPU. 
        Also check if there is a 'Super' or 'Ti' version available for a similar price.
        """
        
        print("[Agent 2] Critiquing build with Market Data...")
        critique_llm_with_tools = critique_llm.bind_tools([web_search])
        response = critique_llm_with_tools.invoke(prompt)
        
        if response.tool_calls:
            print(f"Critique Tool call detected: {response.tool_calls}")
            tool_call = response.tool_calls[0]
            if tool_call['name'] == 'web_search':
                tool_result = web_search.invoke(tool_call['args'])
                final_prompt = f"{prompt}\n\nWeb Search Context: {tool_result}\n\nNow generate the Critique JSON."
                response = critique_llm.invoke(final_prompt)
        
        return parse_json_safely(response.content)
    except Exception as e:
        print(f"Critique Agent error: {e}")
        return {"error": str(e), "stage": "critique"}

async def run_improve_agent(build: dict, critique: dict):
    """Stage 3: Improve Agent - Revise build based on critique"""
    try:
        llm = get_gemini_llm()
        
        prompt = IMPROVE_PROMPT.format(
            build=json.dumps(build.get("build", {})),
            critique=json.dumps(critique.get("critique", {}))
        )
        
        print("[Agent 3] Improving build...")
        response = llm.invoke(prompt)
        
        return parse_json_safely(response.content)
    except Exception as e:
        print(f"Improve Agent error: {e}")
        return {"error": str(e), "stage": "improve"}

def search_pc_parts(query: str):
    """
    Search the internal knowledge base for PC parts, specs, prices, and build advice.
    This tool has access to a database of components (CPU, GPU, etc.) and community build discussions.
    Use this to look up specific part specifications, check prices, or find example builds for similar user requests.
    """
    try:
        if not DATA_STORE_ID or not PROJECT_ID:
            return "Error: Database credentials not configured."
            
        retriever = get_retriever()
        results = retriever.invoke(query)
        
        # Format results for the LLM
        formatted_results = []
        for doc in results:
            # Vertex AI Search returns page_content and metadata
            source = doc.metadata.get("source", "Internal DB") if hasattr(doc, 'metadata') else "Internal DB"
            content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
            formatted_results.append(f"Source: {source}\nContent: {content}\n---")
            
        return "\n".join(formatted_results) if formatted_results else "No results found"
    except Exception as e:
        print(f"Search Error: {e}")
        return f"Error searching database: {str(e)}"


def web_search(query: str):
    """
    Search the public web for recent reviews, benchmarks, or release dates.
    Use this ONLY if the internal database (search_pc_parts) doesn't have the answer.
    Useful for: "RTX 5090 release date", "latest driver issues", "CPU benchmark comparisons 2025".
    """
    # Placeholder for Google Search Grounding or Tavily
    # For now, we return a generic response to encourage using the internal DB first
    return f"Simulated web search results for: {query}. (Web Search integration pending)"


# --- Agent Setup ---

def get_agent():
    """Get the ReAct agent for simple chat mode"""
    try:
        llm = get_gemini_llm()
        
        if not llm:
            # Fallback: return a simple mock agent
            return None
        
        tools = [search_pc_parts, web_search]
        
        system_prompt = """You are an expert PC building assistant named "BuildBuddy".
        
        Help users plan and build their dream PCs. You have access to a vast database of PC parts and community build advice.
        """
        
        # Use LangGraph for modern agent execution
        try:
            from langgraph.prebuilt import create_react_agent
            
            # Create the graph
            app_graph = create_react_agent(llm, tools, prompt=system_prompt)
            return app_graph
        except Exception as e:
            print(f"âš ï¸  Warning: Could not initialize LangGraph agent: {e}")
            return None
            
    except Exception as e:
        print(f"âš ï¸  Warning: Could not initialize agent: {e}")
        return None


# --- API Endpoints ---

class ChatRequest(BaseModel):
    message: str

class BuildRequest(BaseModel):
    query: str
    verbose: bool = False

class BuildResponse(BaseModel):
    build: dict
    reasoning: dict
    status: str

class TextToSpeechRequest(BaseModel):
    text: str
    voice: str = "Nova"  # ElevenLabs voice name

async def run_visualizer_agent(build: dict):
    """Stage 4: Visualizer Agent - Generate PC image"""
    print("[Agent 4] Visualizing build...")
    
    # Extract key components for the prompt
    parts = build.get("revisions", {}).get("revised_build", {}).get("parts", [])
    if not parts:
        parts = build.get("build", {}).get("parts", [])
        
    # Create a rich prompt
    components_str = ", ".join([p.get('name', '') for p in parts[:5]])
    prompt = f"A photorealistic, cinematic shot of a custom gaming PC. High-end components: {components_str}. RGB lighting, tempered glass side panel, sleek cable management, water cooling loop. 8k resolution, unreal engine 5 render, dramatic lighting."
    
    image_url = None
    source = "Vertex AI"

    # 1. Try Vertex AI Imagen (if configured)
    try:
        from vertexai.preview.vision_models import ImageGenerationModel
        model = ImageGenerationModel.from_pretrained("imagen-3.0-generate-001")
        images = model.generate_images(
            prompt=prompt,
            number_of_images=1,
            aspect_ratio="16:9",
            seed=42
        )
        # In a real app, we'd upload this to GCS. 
        # For now, we utilize the fallback since we can't easily serve local bytes to the frontend without a file serving endpoint.
        # However, to prove it works, we'll log it.
        print("Vertex AI Image Generated (Bytes available)")
        # For this hackathon demo, we will prefer the URL-based fallback simply because it's easier to render in the frontend 
        # without setting up a static file server for the generated bytes.
        # But if you want to use it, you'd save `images[0].save("frontend/public/gen_pc.png")` and return `/gen_pc.png`
        
        # Let's actually use the fall back for the URL to be safe and visible immediately
        raise Exception("Proceeding to URL-based generation for easy frontend rendering")
        
    except Exception as e:
        print(f"Vertex AI Imagen skipped/failed ({e}), falling back to Dynamic Keyword Search")
        source = "Unsplash (Dynamic Search)"
        
        # 2. Fallback to Dynamic Keyword Search (Unsplash)
        # We construct a search URL using key visual terms from the Prompt
        # Extract color and key vibe words
        keywords = ["gaming pc", "setup"]
        if "pink" in prompt.lower(): keywords.append("pink")
        if "white" in prompt.lower(): keywords.append("white")
        if "rgb" in prompt.lower(): keywords.append("rgb")
        
        # Join with comma for Unsplash search
        search_query = ",".join(keywords)
        # Add random sig to prevent caching
        import random
        sig = random.randint(0, 1000)
        
        # Use Unsplash Source for random image matching keywords
        image_url = f"https://source.unsplash.com/1600x900/?{search_query}&sig={sig}"
        # Note: source.unsplash.com is deprecated but often redirects. 
        # Better alternative if source is down: Unsplash direct search URL structure or similar service like Pexels
        # Let's use a standard Unsplash search URL pattern that works for embedding
        image_url = f"https://images.unsplash.com/photo-1593640408182-31c70c8268f5?q=80&w=2574&auto=format&fit=crop" # Keeping a safe high-quality default if dynamic fail
        
        # Let's try to find a specific "Pink PC" image if requested, else "White", else "Black/RGB"
        if "pink" in prompt.lower():
             image_url = "https://i.pinimg.com/originals/f3/14/05/f31405edd3df05d045c742fb6e511790.jpg" # Example Pink PC
        elif "white" in prompt.lower():
             image_url = "https://images.unsplash.com/photo-1587202372775-e229f172b9d7?q=80&w=2574" # White/Clean Setup
        else:
             image_url = "https://images.unsplash.com/photo-1603481588273-2f908a9a7a1b?q=80&w=2070" # Dark RGB Setup
             
    return {
        "image_url": image_url,
        "prompt": prompt,
        "source": source
    }

@app.post("/build-pc")
async def build_pc_with_reasoning(request: BuildRequest):
    """Multi-agent pipeline: Build â†’ Critique â†’ Improve â†’ Visualize"""
    try:
        if request.verbose:
            print(f"\n[Pipeline] User Query: {request.query}")
            print("DEBUG: VERSION 1000 - DEEP LOGGING")
        
        # Stage 1: BUILD AGENT
        build = await run_build_agent(request.query)
        if "error" in build:
            raise Exception(f"Build Agent failed: {build.get('error')}")
        
        # Stage 2: CRITIQUE AGENT
        critique = await run_critique_agent(build, request.query)
        if "error" in critique:
            raise Exception(f"Critique Agent failed: {critique.get('error')}")
        
        # Stage 3: IMPROVE AGENT
        revisions = await run_improve_agent(build, critique)
        if "error" in revisions:
            raise Exception(f"Improve Agent failed: {revisions.get('error')}")
            
        # Stage 4: VISUALIZER AGENT (Parallel or Sequential)
        # We pass the final build (revisions or original)
        visualization = await run_visualizer_agent(revisions if revisions.get("revisions") else build)
        
        # Extract final build
        final_build = revisions.get("revisions", {}).get("revised_build", build.get("build", {}))
        
        if request.verbose:
            print(f"[Pipeline] Complete! Final budget: ${final_build.get('total_budget', 'N/A')}")
        
        return BuildResponse(
            build=final_build,
            reasoning={
                "stage_1_build": build,
                "stage_2_critique": critique,
                "stage_3_improvements": revisions,
                "stage_4_visualization": visualization
            },
            status="success"
        )
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat")
async def chat(request: ChatRequest):
    """Legacy endpoint: Simple chat without reasoning pipeline"""
    try:
        agent_graph = get_agent()
        
        if not agent_graph:
            return {
                "response": "âš ï¸ Chat mode is not available. Backend is starting up or Google Cloud is not configured. Please use the Reasoning Mode (/build-pc endpoint) or check your configuration."
            }
        
        # Invoke the graph
        # LangGraph expects {"messages": [("user", "message")]}
        response = agent_graph.invoke({"messages": [("user", request.message)]})
        
        # Extract the final AI message content
        ai_message = response["messages"][-1].content
        return {"response": ai_message}
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/tts")
async def text_to_speech(request: TextToSpeechRequest):
    """Convert text to speech using ElevenLabs"""
    try:
        if not ELEVENLABS_API_KEY:
            raise HTTPException(
                status_code=400, 
                detail="ElevenLabs API key not configured. Set ELEVENLABS_API_KEY environment variable."
            )
        
        from elevenlabs.client import ElevenLabs
        client = ElevenLabs(api_key=ELEVENLABS_API_KEY)
        
        # Generate audio using ElevenLabs
        audio = client.text_to_speech.convert(
            text=request.text,
            voice_id="21m00Tcm4TlvDq8ikWAM",  # Nova voice (default)
            model_id="eleven_monolingual_v1"
        )
        
        # Convert generator to bytes
        audio_bytes = b"".join(audio)
        
        return StreamingResponse(
            iter([audio_bytes]),
            media_type="audio/mpeg",
            headers={"Content-Disposition": "inline; filename=audio.mp3"}
        )
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
def read_root():
    return {
        "status": "PC Part Picker AI Backend running",
        "endpoints": {
            "/build-pc": "POST - Multi-agent reasoning pipeline (Build â†’ Critique â†’ Improve)",
            "/chat": "POST - Simple chat endpoint",
            "/tts": "POST - Text-to-speech using ElevenLabs",
            "/": "GET - Status"
        },
        "note": "Backend is fully operational. Try /build-pc endpoint for multi-agent reasoning."
    }

@app.on_event("startup")
async def startup_event():
    """Print startup messages"""
    print("\n" + "="*70)
    print("  ðŸš€ BuildBuddy AI Backend Started Successfully!")
    print("="*70)
    print(f"  Project ID: {PROJECT_ID if PROJECT_ID else 'âš ï¸  Not configured'}")
    print(f"  Location: {LOCATION}")
    print(f"  Gemini Model: {GEMINI_MODEL}")
    print(f"  Data Store ID: {DATA_STORE_ID if DATA_STORE_ID else 'âš ï¸  Not configured'}")
    print(f"  ElevenLabs: {'âœ… Configured' if ELEVENLABS_API_KEY else 'âš ï¸  Not configured'}")
    print("\n  ðŸ“š API Docs: http://localhost:8000/docs")
    print("  ðŸŒ Frontend: http://localhost:3000")
    print("="*70 + "\n")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=False)
